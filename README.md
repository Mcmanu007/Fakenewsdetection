ğŸ—ï¸ Fake News Detection Project

A machine learning project that uses Natural Language Processing techniques to classify news articles as either real or fake using logistic regression and TF-IDF vectorization.

ğŸ“‹ Overview

This project implements a binary classification system to detect fake news articles. The model uses text preprocessing, TF-IDF vectorization, and logistic regression to achieve accurate classification of news content.

ğŸ“Š Dataset

- ğŸ”— Source: Kaggle dataset by vishakhdapat
- ğŸ“ Dataset: fake-news-detection
- ğŸ“„ File: fake_and_real_news.csv
- ğŸ·ï¸ Features: Text content and binary labels (Real/Fake)

ğŸ“‚ Project Structure

```
fakenew.ipynb          # Main Jupyter notebook with complete implementation
README.md              # Project documentation
Dataset/               # Downloaded from Kaggle (cached locally)
    fake_and_real_news.csv
```

ğŸ”¬ Methodology

1. ğŸ“¥ Data Loading and Exploration
- Downloads dataset using kagglehub
- Loads data using pandas
- Performs initial data exploration and visualization

2. ğŸ§¹ Data Preprocessing
- Duplicate Removal: Identifies and removes duplicate entries
- Null Value Check: Ensures data quality
- Class Distribution Analysis: Visualizes balance between real and fake news

3. ğŸ“ Text Preprocessing
The preprocess_text() function performs:
- Lowercasing: Converts all text to lowercase
- Special Character Removal: Removes non-alphabetic characters
- Tokenization: Splits text into individual words
- Stopword Removal: Removes common English stopwords
- Short Word Filtering: Removes words with length less than or equal to 1

4. âš™ï¸ Feature Engineering
- TF-IDF Vectorization with parameters:
  - max_features=1000: Limits vocabulary size
  - min_df=2: Ignores terms appearing in fewer than 2 documents
  - max_df=0.8: Ignores terms appearing in more than 80% of documents
  - ngram_range=(1,2): Uses unigrams and bigrams
  - sublinear_tf=True: Applies sublinear scaling

5. ğŸ¤– Model Training
- Algorithm: Logistic Regression
- Parameters:
  - max_iter=1000: Maximum iterations for convergence
  - random_state=42: For reproducibility
  - multi_class='multinomial': Multinomial approach
  - solver='lbfgs': Limited-memory BFGS solver

6. ğŸ“ˆ Model Evaluation
- Train-Test Split: 80-20 split with stratification
- Metrics:
  - Accuracy Score
  - Classification Report (Precision, Recall, F1-score)
  - Confusion Matrix with heatmap visualization

ğŸ“¦ Dependencies

```python
import kagglehub
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
```

ğŸ› ï¸ Installation and Setup

1. ğŸ’» Install required packages:
```bash
pip install kagglehub pandas numpy matplotlib seaborn scikit-learn nltk
```

2. ğŸ“š Download NLTK data:
```python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
```

3. â–¶ï¸ Run the notebook:
   - Open fakenew.ipynb in Jupyter Notebook or JupyterLab
   - Execute cells sequentially

ğŸš€ Usage

1. ğŸ“¥ Data Download: The notebook automatically downloads the dataset from Kaggle
2. ğŸ”„ Preprocessing: Text data is cleaned and prepared for modeling
3. ğŸ¯ Training: The logistic regression model is trained on preprocessed text
4. ğŸ“Š Evaluation: Model performance is assessed using multiple metrics
5. ğŸ“ˆ Visualization: Results are displayed through confusion matrix heatmap

âœ¨ Key Features

- ğŸ”„ Automated Data Pipeline: From raw text to trained model
- ğŸ§¹ Comprehensive Preprocessing: Handles text cleaning and normalization
- âš™ï¸ Feature Engineering: TF-IDF vectorization with optimized parameters
- ğŸ“Š Model Evaluation: Multiple metrics for performance assessment
- ğŸ“ˆ Visualization: Clear plots for data distribution and model performance

ğŸ“Š Results

The model provides:
- ğŸ¯ Accuracy score on test data
- ğŸ“‹ Detailed classification report with precision, recall, and F1-scores
- ğŸ”¥ Confusion matrix visualization for performance analysis

ğŸ”® Future Improvements

- ğŸ§  Experiment with other algorithms (Random Forest, SVM, Neural Networks)
- ğŸ“ Implement advanced text preprocessing (stemming, lemmatization)
- âœ… Add cross-validation for more robust evaluation
- ğŸ” Include feature importance analysis
- ğŸŒ Deploy model as a web application

ğŸ‘¨â€ğŸ’» Author

Machine Learning Project - Fake News Detection

ğŸ“„ License

This project is for educational purposes.
